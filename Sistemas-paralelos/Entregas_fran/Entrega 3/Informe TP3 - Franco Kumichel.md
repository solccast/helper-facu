# Informe Trabajo Pr√°ctico 3 | Programaci√≥n en pasaje de mensajes / Programaci√≥n h√≠brida

_Por Franco Kumichel_

### Introducci√≥n

Se propone realizar un informe que describa brevemente las soluciones planteadas, an√°lisis de resultados y conclusiones. El presente se divide en dos partes principales: en la primera se resuelven los ejercicios 2 y 3 correspondientes a la pr√°ctica 4 para los cuales se ejecutan los algoritmos provistos por la c√°tedra y se responden las preguntas planteadas en dichos ejercicios, y luego en la segunda parte se resuelve la expresi√≥n dada desarrollando un algortimo paralelo empleando mpi y otro algoritmo paralelo h√≠brido empleando MPI + OpenMP. De cada caso se analiza su rendimiento y escalabilidad tanto individual como comparativamente y se incluyen tablas con los tiempos de ejecuci√≥n, valores de Speedup, Eficiencia y Overhead de comunicaci√≥n para cada caso.

#### Caracter√≠sticas del hardware y software utilizados:

Cluster provisto por la c√°tedra de Sistemas Paralelos de la Facultad de Informatica UNLP, el cual posee las siguientes caracter√≠sticas:

- 8GB de RAM
- Dos procesadores Intel XeonE5405

El procesador Intel XeonE5405 posee las siguientes caracter√≠sticas:

- Frecuencia b√°sica de 2.0GHz
- Cach√©: 12MB L2 Cach√©
- N√∫mero de cores f√≠sicos: 4
- Hilos por core: 4
- Velocidad del bus: 1333MHz

### Parte 1
#### Ejercicio 2
Los c√≥digos blocking.c y non-blocking.c siguen el patr√≥n master-worker, donde los procesos worker le env√≠an un mensaje de texto al master empleando operaciones de comunicaci√≥n bloqueantes y no bloqueantes, respectivamente.

- Compile y ejecute ambos c√≥digos usando P={4,8,16} (no importa que el n√∫mero de n√∫cleos sea menor que la cantidad de procesos). ¬øCu√°l de los dos retorna antes el control? 
<br>

    A continuaci√≥n se muestran las salidas para _blocking.c_ y _non-blocking.c_

    ```bash
    Tiempo transcurrido 0.000002 (s):       proceso 0, llamando a MPI_Recv() [bloqueante] (fuente rank 1)
    Tiempo transcurrido 2.000265 (s):       proceso 0, MPI_Recv() devolvio control con mensaje: Hola Mundo! Soy el proceso 1
    Tiempo transcurrido 2.000277 (s):       proceso 0, llamando a MPI_Recv() [bloqueante] (fuente rank 2)
    Tiempo transcurrido 4.000113 (s):       proceso 0, MPI_Recv() devolvio control con mensaje: Hola Mundo! Soy el proceso 2
    Tiempo transcurrido 4.000128 (s):       proceso 0, llamando a MPI_Recv() [bloqueante] (fuente rank 3)
    Tiempo transcurrido 6.000145 (s):       proceso 0, MPI_Recv() devolvio control con mensaje: Hola Mundo! Soy el proceso 3

    Tiempo total = 0.000000 (s)
    ```

    ```bash
    Tiempo transcurrido 0.000001 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 1)
    Tiempo transcurrido 0.000048 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 1.999974 (s):       proceso 0, operacion receive completa con mensaje: Hola Mundo! Soy el proceso 1
    Tiempo transcurrido 1.999987 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 2)
    Tiempo transcurrido 1.999995 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 3.999921 (s):       proceso 0, operacion receive completa con mensaje: Hola Mundo! Soy el proceso 2
    Tiempo transcurrido 3.999933 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 3)
    Tiempo transcurrido 3.999940 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 6.000016 (s):       proceso 0, operacion receive completa con mensaje: Hola Mundo! Soy el proceso 3

    Tiempo total = 0.000000 (s)

    ```

En las salidas mostradas anteriormente se puede observar la diferencia entre _MPI_Recv()_ y _MPI_IRecv()_, ya que el primero no retorna el control hasta que no se haya recibido y leido el mensaje solicitado completamente al buffer. Sin embargo el segundo retorna el control inmediatamente, requiriendo mas adelante llamar a _MPI_Wait()_ sobre la petici√≥n realizada _MPI_IRecv()_ para esperar a que se termine de leer el mensaje.

- En el caso de la versi√≥n no bloqueante, ¬øqu√© sucede si se elimina la operaci√≥n MPI_Wait() (l√≠nea 52)? ¬øSe imprimen correctamente los mensajes enviados? ¬øPor qu√©?
<br>

    Salida de _non-blocking.c_ eliminando la operaci√≥n _MPI_Wait()_ (linea 52):
    <br>
    ```bash
    Tiempo transcurrido 0.000001 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 1)
    Tiempo transcurrido 0.000044 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 0.000060 (s):       proceso 0, operacion receive completa con mensaje: No deberia estar leyendo esta frase.
    Tiempo transcurrido 0.000067 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 2)
    Tiempo transcurrido 0.000077 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 0.000094 (s):       proceso 0, operacion receive completa con mensaje: No deberia estar leyendo esta frase.
    Tiempo transcurrido 0.000103 (s):       proceso 0, llamando a MPI_IRecv() [no bloqueante] (fuente rank 3)
    Tiempo transcurrido 0.000113 (s):       proceso 0, MPI_IRecv() devolvio el control..
                                            ..pero el mensaje no fue aun recibido..
    Tiempo transcurrido 0.000125 (s):       proceso 0, operacion receive completa con mensaje: No deberia estar leyendo esta frase.

    Tiempo total = 0.000000 (s)

    ```

    Los mensajes no se imprimen correctamente. En vez de imprimir los mensajes enviados por los procesos, se puede lee en la salida _no deber√≠a estar leyendo esta frase_, el cual es el valor inicial asignado al buffer. Esto se debe a que al no esperar la llegada del mensaje mediante _MPI_Wait()_, el proceso _MASTER_ imprime inmediatamente el contenido del buffer que no se actualiz√≥ con el mensaje esperado.

#### Ejercicio 3
Los c√≥digos blocking-ring.c y non-blocking-ring.c comunican a los procesos en forma de anillo empleando operaciones bloqueantes y no bloqueantes, respectivamente. Compile y ejecute ambos c√≥digos empleando P={4,8,16} (no importa que el n√∫mero de n√∫cleos sea menor que la cantidad de procesos) y N={10000000, 20000000, 40000000, ‚Ä¶}. ¬øCu√°l de los dos algoritmos requiere menos tiempo de comunicaci√≥n? ¬øPor qu√©?

Se han compilado y ejecutado ambos algoritmos variando el tama√±o de la matriz N y la cantidad de procesos P, obteniendo los siguientes resultados:

**blocking-ring:**

|P/N  |10000000|20000000|40000000|
|:---:|  :---: | :---:  | :---:  |
|  4  |0.248836|0.495090|0.985015|
|  8  |0.570116|1.137427|2.308255|
|  16 |2.369974|4.720051|9.579285|

**non-blocking-ring:**

|P/N  |10000000|20000000|40000000|
|:---:|  :---: | :---:  | :---:  |
|  4  |0.220926|0.442457|0.878886|
|  8  |0.300627|0.592935|1.179835|
|  16 |0.966698|1.931252|3.907735|

Se puede observar en los resultados obtenidos que el algoritmo no bloqueante (_non-blocking-ring.c_) requiere menos tiempo de comunicaci√≥n. Observando el c√≥digo podemos inferir que la diferencia est√° en el uso de _MPI_Recv()_ y _MPI_Send()_, las cuales son bloqueantes, contra _MPI_IRecv()_ y _MPI_Isend()_, que son no bloqueantes, para enviar los vectores _sendbuff_ y _recvbuff_. En _blocking-ring.c_ todos los procesos menos el √∫ltimo realizan en primer lugar _MPI_Recv()_ y luego _MPI_Send()_. Al ser _MPI_Recv()_ bloqueante, estos procesos no pueden comenzar a enviar sus vectores _sendbuff_ hasta que no reciban completamente el vector _recvbuff_. Por otro lado, el algortimo _non-blocking-ring.c_ utiliza versiones no bloqueantes para el envio y recepci√≥n de mensajes, por lo que no tienen ete problema, pueden enviar y recibir ambos vectores en simult√°neo y luego esperar que ambas operaciones finalicen.

### Parte 2

Dada la siguiente expresi√≥n:

$ùëÖ = \dfrac{(ùëÄùëéùë•ùê¥ √ó ùëÄùëéùë•ùêµ ‚àí ùëÄùëñùëõùê¥ √ó ùëÄùëñùëõùêµ)}{ùëÉùëüùëúùëöùê¥ √ó ùëÉùëüùëúùëöùêµ} √ó [ùê¥ √ó ùêµ] + [ùê∂ √ó ùê∑]$

- Donde $A, B, C, D$ y $R$ son matrices cuadradas de $NxN$ con elementos de tipo double.
- $MaxA, MinA$ y $PromA$ son los valores m√°ximo, m√≠nimo y promedio de la matriz A, respectivamente.
- $MaxB, MinB$ y $PromB$ son los valores m√°ximo, m√≠nimo y promedio de la matriz $B$, respectivamente.

Desarrolle 2 algoritmos que computen la expresi√≥n dada:
1. Algoritmo paralelo empleando MPI
2. Algoritmo paralelo h√≠brido empleando MPI+OpenMP

Mida el tiempo de ejecuci√≥n de los algoritmos en el cluster remoto. Las prueban deben considerar la variaci√≥n del tama√±o de problema (N={512, 1024, 2048, 4096}) y, en el caso de los algoritmos paralelos, tambi√©n la cantidad de n√∫cleos (P={8,16,32} para MPI, es decir, 1, 2 y 4 nodos, respectivamente; P={16,32} para h√≠brido, es decir, 2 y 4 nodos, respectivamente).

Para la implementaci√≥n de los algoritmos solicitados, se tuvieron en cuenta los siguientes aspectos:

#### Algoritmo paralelo utilizando MPI
Esta soluci√≥n se basa en la librer√≠a **OpenMPI**, la cual utiliza el estandar MPI (Message Passing Interface). En este caso las operaciones utilizadas son:

- *MPI_Barrier()*: se utiliza para sincronizar los procesos para poder medir los tiempos correctamente.

- *MPI_Scatter()*: se utiliza para enviar las matrices que se ordenan por filas. Esto se debe a que cada proceso debe tener una parte de la matriz para aplicarle las operaciones que le corresponda.

- *MPI_Bcast()*: se utiliza para enviar las matrices que se ordenan por columnas. Esto se debe a que los procesos necesitan la matriz entera, para poder acceder a todas sus columnas al realizar la multiplicaci√≥n de matrices.

- *MPI_Allreduce()*: se utiliza para poder recibir el m√°ximo, m√≠nimo y suma que encontr√≥ cada proceso de las matrices $A$ y $B$, con el fin de obtener el valor correspondiente.

- *MPI_Gather()*: se utiliza para que luego que todos los procesos hayan ejecutado las operaciones necesarias, el proceso coordinador obtenga todas las partes de la matriz resultante.

#### Algoritmo paralelo h√≠brido utilizando MPI + OpenMP
Esta soluci√≥n h√≠brida combina la memoria compartida con el pasaje de mensajes aprovechando las potencialidades de cada uno. La comunicaci√≥n entre hilos que pertenecen a la misma maquina f√≠sica se realiza utilizando memoria compartida mientras que la comunicaci√≥n entre ellas se lleva a cabo por medio de pasaje de mensajes. 

El algoritmo implementa un enfoque de interacci√≥n master/worker, donde el master asume tanto el rol de coordinador como de trabajador. Se utilizan directivas de OMP (OpenMP) para controlar el paralelismo a nivel de hilo dentro de cada proceso, mientras que las comunicaciones de MPI se utilizan para sincronizar y compartir datos entre los distintos procesos.

Al inicio, el coordinador distribuye las matrices a los distintos procesos utilizando comunicaciones colectivas. Como todos los procesadores tienen la misma capacidad de c√≥mputo y los bloques son del mismo tama√±o, los procesos trabajan a la misma velocidad, reparti√©ndose equitativamente las filas de las matrices.

Luego, cada proceso entra en una regi√≥n paralela con la macro *#pragma omp parallel* para realizar los c√°lculos con m√∫ltiples hilos. La macro *#pragma omp single* asegura que una secci√≥n del c√≥digo se ejecute una sola vez por proceso, facilitando que solo un proceso por nodo se comunique con los dem√°s.

Finalmente, tras completar las operaciones en paralelo, los resultados se re√∫nen en el proceso coordinador mediante la comunicaci√≥n colectiva *MPI Gather*.

#### Resultados de ejecuci√≥n:

Los algoritmos se ejecutaron con las siguiente caracter√≠sticas:

- Se utilizo la optimizaci√≥n O3 ya que fue la que dio resultados m√°s favorables en cuanto a performance y la precisi√≥n matem√°tica que brinda.

- En la medida de lo posible, se utilizo como tama√±o de bloque 128 que fue el que mejores tiempos de ejecuci√≥n brindo seg√∫n se vio en las anteriores entregas. En los casos donde no fue posible utilizar este valor de bloque, se opto por utilizar el segundo que dio mejores resultados, sino el tercero y asi prosiguiendo.

**Algoritmo secuencial**

Tiempos de ejecuci√≥n:

| N   |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
| Tiempo ejecuci√≥n    | 0.427138 | 3.405070 |27.118076 |216.650521 |

**Algoritmo paralelo con OpenMP**

Tiempos de ejecuci√≥n:

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  2  | 0.230495 | 1.783829 |14.155644|113.043257|
|  4  | 0.116238 | 0.894773 |7.100621 |56.878447 |
|  8  | 0.064789 | 0.462704 |3.638109 |29.648739 |

**Algoritmo paralelo con MPI:**

Tiempos de ejecuci√≥n total:

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  8  | 0.091785 | 0.553571 |4.103143 |32.336629 |
|  16 | 0.107200 | 0.569465 |3.028158 |20.361223 |
|  32 | 0.119979 | 0.570909 |2.601665 |13.568427 |

Tiempos de comunicaci√≥n:

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  8  | 0.031544 | 0.091479 |0.387143 |1.471631  |
|  16 | 0.087693 | 0.341555 |1.212650 |5.133080  |
|  32 | 0.121441 | 0.379243 |1.515286 |5.889797  |


**Algoritmo paralelo h√≠brido (MPI + OpenMP):**

Tiempos de ejecuci√≥n total:

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  16 | 0.107698 | 0.525190 |2.949862 |19.267635 |
|  32 | 0.111222 | 0.494151 |2.357711 |13.296873 |

Tiempos de comunicaci√≥n:

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  16 | 0.073313 | 0.283945 |1.119810 |4.447214  |
|  32 | 0.095832 | 0.352190 |1.380991 |5.905809  |

#### Speedup, Eficiencia y Overhead de comunicaci√≥n

El **Speedup** es la relaci√≥n entre el tiempo de ejecuci√≥n de una tarea en un sistema secuencial y el tiempo de ejecuci√≥n de la misma tarea en un sistema paralelo. Para calcular el mismo se utiliza la siguiente relaci√≥n:

$S_{p}(n) = \dfrac{T_{s}(n)}{T_{p}(n)}$

La **eficiencia** mide la relaci√≥n entre el tiempo empleado y la cantidad de recursos utilizados para realizar una tarea espec√≠fica en un entorno paralelo. Una alta eficiencia indica que el sistema est√° aprovechando de manera √≥ptima sus recursos para completar las tareas. Este se calcula de la siguiente manera:

$E_{p}(n) = \dfrac{S_{p}(n)}{S_{optimo}}$

El overhead de comunicaci√≥n la relaci√≥n entre el tiempo requerido por las comunicaciones de nuestra soluci√≥n y el tiempo total que esta requiera. Se calcula con la siguiente formula:

$OC_{p}(n) = \dfrac{Tcomm_{p}(n)}{T_{p}(n)} \times 100$

**Algoritmo paralelo con OpenMP**

Speedup

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  2  | 1.853133 | 1.908854 |1.915707 |1.916527  |
|  4  | 3.674684 | 3.805512 |3.819113 |3.809009  |
|  8  | 6.592754 | 7.359067 |7.453893 |7.307242  |

Eficiencia

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  2  | 0.926566 | 0.954427 |0.957853 |0.958263  |
|  4  | 0.918671 | 0.951378 |0.954778 |0.952252  |
|  8  | 0.824094 | 0.919883 |0.931736 |0.913405  |

**Algoritmo paralelo con MPI**

Speedup

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  8  | 4.653679 | 6.151098 |6.609098 |6.699848  |
|  16 | 3.984496 | 5.979419 |8.955304 |10.640349 |
|  32 | 3.560106 | 5.964295 |10.423354|15.967254 |

Eficiencia

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  8  | 0.581709 | 0.768887 |0.826137 |0.837481  |
|  16 | 0.249031 | 0.373713 |0.559706 |0.665021  |
|  32 | 0.111253 | 0.186384 |0.325729 |0.498997  |

Overhead por comunicaci√≥n

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  8  | 0.343672 | 0.165252 |0.094352 |0.045509  |
|  16 | 0.818031 | 0.599782 |0.400457 |0.252100  |
|  32 | 1.000000 | 0.664279 |0.582429 |0.434081  |

**Algoritmo paralelo h√≠brido (MPI + OpenMP)**

Speedup

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  16 | 3.966071 | 6.483501 |9.192998 |11.244271 |
|  32 | 3.840409 | 6.890747 |11.501866|16.293343 |

Eficiencia

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  16 | 0.247879 | 0.405218 |0.574562 |0.702766  |
|  32 | 0.120012 | 0.215335 |0.359433 |0.509166  |

Overhead por comunicaci√≥n

|P/N  |   512    |   1024   |  2048   |   4096   |
|:---:|  :---:   |  :---:   |  :---:  |  :---:   |
|  16 | 0.680727 | 0.540651 |0.379614 |0.230812  |
|  32 | 0.861628 | 0.712717 |0.585733 |0.444150  |

#### An√°lisis de los resultados

**MPI vs OpenMP**

En las tablas de los tiempos se observa que el algoritmo que utiliza OpenMP es mas r√°pido. Como consecuencia de esto, el SpeedUp y por √∫ltimo la eficiencia de este es mejor en comparaci√≥n del algoritmo con MPI.

**MPI vs H√≠brido (MPI+OpenMP)**

Al analizar las tablas, se puede ver que los tiempos de comunicaci√≥n son mayores en MPI. Esto sucede porque se tienen mas procesos que deben comunicarse a trav√©s del pasaje de mensajes. Por ejemplo para P=32, en MPI se tienen 32 procesos que deben comunicarse entre si usando pasaje de mensajes.
En cambio en el h√≠brido, solamente se crean 4 procesos, que luego cada uno crea 8 threads, que utilizan memoria compartida, as√≠ disminuyendo el pasaje de mensajes. Como consecuencia a que los tiempos del h√≠brido son mejores, el speedUp y la eficiencia de este tambi√©n tienen resultados mas √≥ptimos.

En los gr√°ficos de eficiencia y overhead de comunicaci√≥n tanto de MPI como del h√≠brido se puede ver que mas grande son las barras que indican el overhead, m√°s se achica el de la eficiencia. Que esto suceda es muy coherente, ya que a mayor tiempo se gaste realizando las comunicaciones, menor tiempo
se van a estar empleando las unidades de procesamiento para realizar computo. Se puede constatar que al incrementar el N, al tener que realizar mas computo, el overhead se decrementa y la eficiencia aumenta.

**Rendimiento y escalabilidad**

**Rendimiento**: esta caracter√≠stica de los programas paralelos se puede evaluar, analizando el SpeedUp y teniendo en cuenta las dos leyes:
- *Ley de Amdahl:* Permite estimar el Speedup alcanzable en aquellos programas paralelos que
contienen partes secuenciales.
- *Ley de Gustafson:* Reescribi√≥ la ecuaci√≥n que plante√≥ Amdahl para estimar el SpeedUp m√°ximo
alcanzable, pero teniendo en cuenta el tama√±o del problema (N).

**Escalabilidad:** esta caracter√≠stica de los programas paralelos hace referencia a la capacidad que tiene un sistema de mantener un nivel de eficiencia fijo al incrementar tanto el numero de unidades de procesamiento (P), como el tama√±o del problema a resolver (N).

**Algoritmo paralelo MPI**

*Rendimiento:*
- Ley de Amdahl: en la tabla se puede ver que al incrementar el P, y mantener fijo el N, el speedUp mejora (excepto en valores con N chico, ya que se produce un gran overhead de comunicaci√≥n).
Esto es porque al utilizar una mayor cantidad de unidades de procesamiento la soluci√≥n al problema se obtiene en un tiempo menor.

- Ley de Gustafson: al aumentar el tama√±o del problema (N) y mantener el P tambi√©n mejora el speedUp. Esto sucede porque la proporci√≥n de c√≥digo que se ejecuta en paralelo, crece mayormente a la secci√≥n secuencial.

**Escalabilidad**:

- No es fuertemente escalable: debido a que si incrementamos el valor de P, y mantenemos fijo el valor de N, la eficiencia no se mantiene en un valor fijo.

- Para analizar si es d√©bilmente escalable podemos mirar en diagonal los valores de la eficiencia. Si quisi√©ramos hacer una comparaci√≥n equitativa entre MPI y el h√≠brido deber√≠amos mirar la tabla solo cuando P=16 y P=32. Mirando en estos valores podemos concluir que es d√©bilmente escalable, ya que en una de las diagonales vemos que la eficiencia se mantiene entre el 55% y 49%. Se concluye entonces que el algoritmo realizado con MPI, es d√©bilmente escalable.

**Algoritmo H√≠brido (MPI + OpenMP)**
*Rendimiento*:
- Ley de Amdahl: en la tabla se puede ver que al incrementar el P, y mantener fijo el N, el speedUp mejora. Esto es porque al utilizar una mayor cantidad de unidades de procesamiento la soluci√≥n al problema se obtiene en un tiempo menor.

- Ley de Gustafson: al aumentar el tama√±o del problema (N) y mantener el P tambi√©n mejora el speedUp. Esto sucede porque la proporci√≥n de c√≥digo que se ejecuta en paralelo, crece mayormente a la secci√≥n secuencial.

**Escalabilidad:**

- No es fuertemente escalable: debido a que si incrementamos el valor de P, y mantenemos fijo el valor de N, la eficiencia no se mantiene en un valor fijo.

- Si es d√©bilmente escalable: si incrementamos el valor de P y adem√°s incrementamos el N (como por ejemplo con ‚ÄúP=16 y N=1024‚Äù con ‚ÄúP=32 y N=2048‚Äù), se puede ver que la eficiencia se mantiene aproximadamente en un 40-35%.

#### Conclusi√≥n

En este trabajo se compararon tres soluciones paralelas (MPI, OpenMP y MPI+OpenMP) para resolver la ecuaci√≥n propuesta, contrast√°ndolas tambi√©n con una soluci√≥n secuencial. 

Uno de los aspectos en com√∫n del algoritmo h√≠brido y el que utiliza MPI es el impacto del overhead de las comunicaciones. Cuando se trabaja con matrices peque√±as y un P elevado se observa una reducci√≥n considerable en el SpeedUp debido a este factor. En el caso de OpenMP esto sucede debido al costo de
sincronizaci√≥n entre los hilos.

La soluci√≥n h√≠brida que combina MPI y OpenMP resulta la m√°s prometedora, ya que logra aprovechar las ventajas de ambos enfoques (memoria compartida junto con comunicaci√≥n por mensajes) y muestra una mejor escalabilidad.

La elecci√≥n de la soluci√≥n depende del tama√±o de las matrices y la necesidad de escalabilidad. Para matrices peque√±as, OpenMP es m√°s eficiente debido a su mejor aprovechamiento de la memoria compartida. Para matrices grandes y escenarios que requieran alta escalabilidad y uso de m√∫ltiples nodos, la soluci√≥n h√≠brida es probablemente la m√°s adecuada.